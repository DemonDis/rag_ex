# RAG (Retrieval-Augmented Generation)

RAG — это подход в обработке естественного языка (NLP), который улучшает работу больших языковых моделей (LLM), дополняя их внешними источниками знаний. Вместо того чтобы полагаться только на данные, на которых модель была обучена, RAG позволяет модели получать актуальную и специфическую информацию из внешних баз данных или документов перед генерацией ответа.

## Как работает RAG?

1.  **Поиск (Retrieval):** Когда поступает запрос, система RAG сначала ищет релевантную информацию во внешней базе знаний (например, в базе данных документов, векторной базе данных).
2.  **Дополнение (Augmentation):** Найденная информация (контекст) передается вместе с исходным запросом в LLM.
3.  **Генерация (Generation):** LLM использует полученный контекст для генерации более точного, релевантного и обоснованного ответа.

## Преимущества RAG:

*   **Актуальность:** Обеспечивает доступ к самой свежей информации, преодолевая проблему устаревания знаний LLM.
*   **Точность и обоснованность:** Ответы основаны на конкретных данных, что снижает вероятность "галлюцинаций" модели.
*   **Специфичность:** Позволяет моделям отвечать на вопросы, требующие знаний из узкоспециализированных областей, не включенных в основное обучение.
*   **Экономичность:** Часто более эффективно, чем дообучение (fine-tuning) LLM для каждого нового набора данных.

## Что существует помимо RAG?

Помимо RAG, существует множество других подходов и технологий для улучшения работы с LLM и обработки информации:

*   **Fine-tuning (Дообучение):** Адаптация LLM путем дополнительного обучения на специфических наборах данных. Позволяет модели лучше понимать стиль, тон и терминологию конкретной области, но может быть дорогостоящим и требовать больших объемов данных.
*   **Prompt Engineering:** Искусство составления запросов (промптов) для LLM с целью получения наилучшего результата. Включает техники, такие как few-shot learning (предоставление примеров в запросе).
*   **Векторные базы данных:** Специализированные базы данных, оптимизированные для хранения и поиска векторных представлений данных. Являются ключевым компонентом многих RAG-систем.
*   **Агенты (Agents):** Системы, которые могут использовать LLM для планирования и выполнения последовательности действий, включая использование различных инструментов (поиск в интернете, выполнение кода, доступ к API).

RAG является мощным инструментом для повышения качества и надежности ответов LLM, особенно в сценариях, где важна актуальность и точность данных.

## Запуск модели
### Скачивание образа Ollama
```bash
docker pull ollama/ollama
```

### Запуск Ollama в Docker
```bash
docker run -d \
  --name ollama \
  -p 11434:11434 \
  -v ollama-data:/root/.ollama \
  ollama/ollama
```

### Запуск языковой модели через Ollama
Загружает и запускает модель
- nomic-embed-text - векторизация текста **274MB**
- mistral:7b-instruct - генерация ответов **4.1GB**
- qwen2:0.5b-instruct - для быстрых запросов **352MB**
```bash
docker exec ollama ollama pull qwen2:0.5b-instruct
```

```
[React Frontend] 
        ↓ HTTP (POST /ask)
[Python Backend (FastAPI/Flask)] 
        ↓
   [Vector DB (Chroma/FAISS)] ← embeddings from nomic-embed-text
        ↓
   [Ollama (LLM)] ← prompt + retrieved context
        ↓
[Response to React]
```

### Технологический стек
- **Python**: 3.12.3
- **Docker**: 28.3.3, build 980b856
- **Ubuntu**: 24.04