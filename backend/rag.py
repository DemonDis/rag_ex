# backend/rag.py

import re
from rank_bm25 import BM25Okapi
from utils import extract_text
from vector_store import save_chunks, load_chunks
import requests

OLLAMA_API = "http://localhost:11434/api/generate"
CHUNK_SIZE = 500
CHUNK_OVERLAP = 50

def preprocess(text):
    return re.findall(r'\b\w+\b', text.lower())

def split_text_into_chunks(text, chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP):
    chunks = []
    start = 0
    while start < len(text):
        end = start + chunk_size
        chunks.append(text[start:end])
        start = end - overlap
    return chunks

def create_bm25_index(chunks):
    tokenized_corpus = [preprocess(chunk) for chunk in chunks]
    return BM25Okapi(tokenized_corpus)

def retrieve_top_chunks(query, chunks, bm25, top_k=3):
    tokenized_query = preprocess(query)
    scores = bm25.get_scores(tokenized_query)
    top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:top_k]
    return [chunks[i] for i in top_indices]

def process_document(file_path):
    print(f"ðŸ”„ ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ñ„Ð°Ð¹Ð»Ð°: {file_path}")
    text = extract_text(file_path)
    chunks = split_text_into_chunks(text)
    metadata = {"source": file_path}
    save_chunks(chunks, metadata)
    print(f"âœ… Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¾ {len(chunks)} Ñ‡Ð°Ð½ÐºÐ¾Ð²")

def ask_question(question: str, model: str = "qwen2:0.5b-instruct"):
    chunks, metadata = load_chunks()
    if not chunks:
        return "ÐÐµ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ñ‹ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ñ‹. Ð—Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚Ðµ Ñ„Ð°Ð¹Ð»."

    bm25 = create_bm25_index(chunks)
    relevant_chunks = retrieve_top_chunks(question, chunks, bm25, top_k=3)

    context = "\n\n".join(relevant_chunks)

    prompt = f"""
Ð¢Ñ‹ â€” Ð¿Ð¾Ð¼Ð¾Ñ‰Ð½Ð¸Ðº, Ð¾Ñ‚Ð²ÐµÑ‡Ð°ÑŽÑ‰Ð¸Ð¹ Ð½Ð° Ð²Ð¾Ð¿Ñ€Ð¾ÑÑ‹ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ð¿Ñ€ÐµÐ´Ð¾ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð½Ð¾Ð³Ð¾ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð°.
Ð•ÑÐ»Ð¸ Ð¾Ñ‚Ð²ÐµÑ‚Ð° Ð½ÐµÑ‚ Ð² ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ðµ â€” ÑÐºÐ°Ð¶Ð¸, Ñ‡Ñ‚Ð¾ Ð½Ðµ Ð·Ð½Ð°ÐµÑˆÑŒ.

ÐšÐ¾Ð½Ñ‚ÐµÐºÑÑ‚:
{context}

Ð’Ð¾Ð¿Ñ€Ð¾Ñ: {question}
ÐžÑ‚Ð²ÐµÑ‚:
"""

    payload = {
        "model": model,
        "prompt": prompt,
        "stream": False
    }

    # ðŸ”¥ Ð”ÐžÐ‘ÐÐ’Ð¬ Ð­Ð¢Ðž:
    print(f"ðŸ“¡ ÐžÑ‚Ð¿Ñ€Ð°Ð²Ð»ÑÑŽ Ð·Ð°Ð¿Ñ€Ð¾Ñ Ð² Ollama Ð½Ð°: {OLLAMA_API}")
    print(f"ðŸ“„ ÐŸÐ°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹: {payload}")

    try:
        response = requests.post(OLLAMA_API, json=payload, timeout=60)
        response.raise_for_status()
        result = response.json()
        return result.get("response", "ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸.")
    except Exception as e:
        print(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð¾Ð±Ñ€Ð°Ñ‰ÐµÐ½Ð¸Ð¸ Ðº Ollama: {e}")
        return f"ÐžÑˆÐ¸Ð±ÐºÐ° Ollama: {str(e)}"