# backend/rag.py

import re
from rankbm25 import BM25Okapi
from utils import extract_text
from vector_store import save_chunks, load_chunks

OLLAMA_API = "http://host.docker.internal:11434/api/generate"
CHUNK_SIZE = 500
CHUNK_OVERLAP = 50


def preprocess(text):
    """–ü—Ä–æ—Å—Ç–∞—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–ª—è BM25"""
    return re.findall(r'\b\w+\b', text.lower())


def split_text_into_chunks(text, chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP):
    """–†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏ —Å –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ–º"""
    chunks = []
    start = 0
    while start < len(text):
        end = start + chunk_size
        chunks.append(text[start:end])
        start = end - overlap
    return chunks


def create_bm25_index(chunks):
    """–°–æ–∑–¥–∞—ë—Ç BM25 –∏–Ω–¥–µ–∫—Å –¥–ª—è –ø–æ–∏—Å–∫–∞"""
    tokenized_corpus = [preprocess(chunk) for chunk in chunks]
    return BM25Okapi(tokenized_corpus)


def retrieve_top_chunks(query, chunks, bm25, top_k=3):
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç top-k –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤"""
    tokenized_query = preprocess(query)
    scores = bm25.get_scores(tokenized_query)
    top_indices = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:top_k]
    return [chunks[i] for i in top_indices]


def process_document(file_path):
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–π —Ñ–∞–π–ª: –ø–∞—Ä—Å–∏—Ç ‚Üí —Ä–∞–∑–±–∏–≤–∞–µ—Ç ‚Üí —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç"""
    print(f"üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–∞: {file_path}")
    text = extract_text(file_path)
    chunks = split_text_into_chunks(text)
    metadata = {"source": file_path}

    save_chunks(chunks, metadata)
    print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤")


def ask_question(question: str, model: str = "mistral:7b-instruct"):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å, –∏—Å–ø–æ–ª—å–∑—É—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —á–∞–Ω–∫–∏"""
    chunks, metadata = load_chunks()
    if not chunks:
        return "–ù–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –¥–æ–∫—É–º–µ–Ω—Ç—ã. –ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª."

    # –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å BM25 (–º–æ–∂–Ω–æ –∫—ç—à–∏—Ä–æ–≤–∞—Ç—å, –Ω–æ –¥–ª—è –º–∞–ª–æ–≥–æ —á–∏—Å–ª–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ ‚Äî –±—ã—Å—Ç—Ä–æ)
    bm25 = create_bm25_index(chunks)
    relevant_chunks = retrieve_top_chunks(question, chunks, bm25, top_k=3)

    context = "\n\n".join(relevant_chunks)

    prompt = f"""
–¢—ã ‚Äî –ø–æ–º–æ—â–Ω–∏–∫, –æ—Ç–≤–µ—á–∞—é—â–∏–π –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.
–ï—Å–ª–∏ –æ—Ç–≤–µ—Ç–∞ –Ω–µ—Ç –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ ‚Äî —Å–∫–∞–∂–∏, —á—Ç–æ –Ω–µ –∑–Ω–∞–µ—à—å.

–ö–æ–Ω—Ç–µ–∫—Å—Ç:
{context}

–í–æ–ø—Ä–æ—Å: {question}
–û—Ç–≤–µ—Ç:
"""

    payload = {
        "model": model,
        "prompt": prompt,
        "stream": False
    }

    try:
        response = requests.post(OLLAMA_API, json=payload, timeout=30)
        response.raise_for_status()
        result = response.json()
        return result.get("response", "–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏.")
    except Exception as e:
        return f"–û—à–∏–±–∫–∞ Ollama: {str(e)}"