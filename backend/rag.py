# backend/rag.py

import re
import json
import numpy as np
from utils import extract_text
from vector_store import save_chunks, load_chunks
import requests

OLLAMA_API = "http://localhost:11434/api/generate"
EMBEDDING_MODEL = "nomic-embed-text"
CHUNK_SIZE = 500
CHUNK_OVERLAP = 50


def preprocess(text):
    return re.findall(r'\b\w+\b', text.lower())


def split_text_into_chunks(text, chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP):
    chunks = []
    start = 0
    while start < len(text):
        end = start + chunk_size
        chunks.append(text[start:end])
        start = end - overlap
    return chunks


def get_embedding(text, model=EMBEDDING_MODEL):
    """ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÑ‚ ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³ Ñ‚ÐµÐºÑÑ‚Ð° Ñ‡ÐµÑ€ÐµÐ· Ollama"""
    try:
        response = requests.post(
            f"http://localhost:11434/api/embeddings",
            json={"model": model, "prompt": text},
            timeout=10
        )
        response.raise_for_status()
        embedding = response.json().get("embedding")
        if not embedding:
            raise ValueError("Empty embedding")
        return embedding
    except Exception as e:
        print(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð°: {e}")
        return [0.0] * 768  # fallback Ñ€Ð°Ð·Ð¼ÐµÑ€Ð½Ð¾ÑÑ‚ÑŒ nomic-embed-text


def cosine_similarity(a, b):
    """Ð’Ñ‹Ñ‡Ð¸ÑÐ»ÑÐµÑ‚ ÐºÐ¾ÑÐ¸Ð½ÑƒÑÐ½Ð¾Ðµ ÑÑ…Ð¾Ð´ÑÑ‚Ð²Ð¾ Ð¼ÐµÐ¶Ð´Ñƒ Ð´Ð²ÑƒÐ¼Ñ Ð²ÐµÐºÑ‚Ð¾Ñ€Ð°Ð¼Ð¸"""
    a = np.array(a)
    b = np.array(b)
    dot_product = np.dot(a, b)
    norm_a = np.linalg.norm(a)
    norm_b = np.linalg.norm(b)
    if norm_a == 0 or norm_b == 0:
        return 0.0
    return dot_product / (norm_a * norm_b)


def retrieve_top_chunks(query, chunks, embeddings, top_k=3):
    """ÐÐ°Ñ…Ð¾Ð´Ð¸Ñ‚ top-k Ð½Ð°Ð¸Ð±Ð¾Ð»ÐµÐµ Ñ€ÐµÐ»ÐµÐ²Ð°Ð½Ñ‚Ð½Ñ‹Ñ… Ñ‡Ð°Ð½ÐºÐ¾Ð² Ð¿Ð¾ ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð°Ð¼"""
    query_embedding = get_embedding(query)

    similarities = []
    for i, emb in enumerate(embeddings):
        sim = cosine_similarity(query_embedding, emb)
        similarities.append((i, sim))

    # Ð¡Ð¾Ñ€Ñ‚Ð¸Ñ€ÑƒÐµÐ¼ Ð¿Ð¾ ÑÑ…Ð¾Ð¶ÐµÑÑ‚Ð¸ (Ð¿Ð¾ ÑƒÐ±Ñ‹Ð²Ð°Ð½Ð¸ÑŽ)
    similarities.sort(key=lambda x: x[1], reverse=True)
    top_indices = [idx for idx, _ in similarities[:top_k]]
    return [chunks[i] for i in top_indices]


def process_document(file_path):
    print(f"ðŸ”„ ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ñ„Ð°Ð¹Ð»Ð°: {file_path}")
    text = extract_text(file_path)
    chunks = split_text_into_chunks(text)
    metadata = [{"source": file_path} for _ in chunks]

    print("ðŸ§  Ð“ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÑŽ ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð¸ Ð´Ð»Ñ Ñ‡Ð°Ð½ÐºÐ¾Ð²...")
    embeddings = []
    for chunk in chunks:
        emb = get_embedding(chunk)
        embeddings.append(emb)

    save_chunks(chunks, metadata, embeddings)
    print(f"âœ… Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¾ {len(chunks)} Ñ‡Ð°Ð½ÐºÐ¾Ð² Ñ ÑÐ¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð°Ð¼Ð¸")


def ask_question(question: str, model: str = "mistral:7b-instruct"):
    chunks, metadata, embeddings = load_chunks()
    if not chunks:
        return {"answer": "ÐÐµ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ñ‹ Ð´Ð¾ÐºÑƒÐ¼ÐµÐ½Ñ‚Ñ‹. Ð—Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚Ðµ Ñ„Ð°Ð¹Ð».", "sources": []}

    print("ðŸ” Ð˜Ñ‰Ñƒ Ñ€ÐµÐ»ÐµÐ²Ð°Ð½Ñ‚Ð½Ñ‹Ðµ Ñ„Ñ€Ð°Ð³Ð¼ÐµÐ½Ñ‚Ñ‹...")
    relevant_chunks = retrieve_top_chunks(question, chunks, embeddings, top_k=3)

    context = "\n\n".join(relevant_chunks)

    prompt = f"""
Ð¢Ñ‹ â€” Ð¿Ð¾Ð¼Ð¾Ñ‰Ð½Ð¸Ðº, Ð¾Ñ‚Ð²ÐµÑ‡Ð°ÑŽÑ‰Ð¸Ð¹ Ð½Ð° Ð²Ð¾Ð¿Ñ€Ð¾ÑÑ‹ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ð¿Ñ€ÐµÐ´Ð¾ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð½Ð¾Ð³Ð¾ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð°.
Ð•ÑÐ»Ð¸ Ð¾Ñ‚Ð²ÐµÑ‚Ð° Ð½ÐµÑ‚ Ð² ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ðµ â€” ÑÐºÐ°Ð¶Ð¸, Ñ‡Ñ‚Ð¾ Ð½Ðµ Ð·Ð½Ð°ÐµÑˆÑŒ.

ÐšÐ¾Ð½Ñ‚ÐµÐºÑÑ‚:
{context}

Ð’Ð¾Ð¿Ñ€Ð¾Ñ: {question}
ÐžÑ‚Ð²ÐµÑ‚:
"""

    payload = {
        "model": model,
        "prompt": prompt,
        "stream": False
    }

    try:
        response = requests.post(OLLAMA_API, json=payload, timeout=60)
        response.raise_for_status()
        result = response.json()
        answer = result.get("response", "ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸.")
    except Exception as e:
        print(f"âŒ ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¸ Ð¾Ð±Ñ€Ð°Ñ‰ÐµÐ½Ð¸Ð¸ Ðº Ollama: {e}")
        answer = f"ÐžÑˆÐ¸Ð±ÐºÐ° Ollama: {str(e)}"

    # Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÐ¼ ÑƒÐ½Ð¸ÐºÐ°Ð»ÑŒÐ½Ñ‹Ðµ Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¸
    sources = list(set([m["source"] for m in metadata]))

    return {
        "answer": answer,
        "sources": sources
    }